{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is coming from sklearn, already a clean data set so move along to split data into training sets and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data #each list represents an iris flower by some parameters, capital X is standard\n",
    "y = iris.target #types of iris in this case, this is our answer\n",
    "#we want to get each feature\n",
    "feature_names = iris.feature_names #prebuilt with set\n",
    "target_names = iris.target_names\n",
    "#once we understand the data we can split into sets, object is a numpy array\n",
    "from sklearn.model_selection import train_test_split #library documentation- splits arrays into random train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4)\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape) #returns columns and rows, 90 rows, 4 columns, can change test size, we want more train than test for better model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time to creat model. SciKit Learn has some models for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try using the nearest neighbor classification, tries to divide the data into segments. So many to chose, good machine learning scientist knows how to choose\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred= knn.predict(X_test) #now compare y predictions with actual answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics #compare and see our accurace\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#summary, imported and split the data, created a model using scikit nearest neighbor, we checked output with x andy variables, now we can improve our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we split data into test and train, we as an engineer can decide how much to be test and train, the more data the more we can train our models\n",
    "#another is with parameters of KNN is to change to 4 (this actually lessens, it creates 4 and we only need 3 classifications)\n",
    "#perhaps have more columns or parameters (features), means that if there is another way to distiguish, could be a better model\n",
    "#Look at a different algorithm, scikit eg -decision tree\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# knn = DecisionTreeClassifier()\n",
    "# knn.fit(X_train,y_train)\n",
    "# y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# from sklearn import metrics #compare and see our accurace\n",
    "# print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:  ['versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "sample = [[3,5,4,2],[2,3,5,4]]\n",
    "predictions = knn.predict(sample)\n",
    "pred_species = [iris.target_names[p] for p in predictions]\n",
    "print(\"predictions: \", pred_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kristen\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#hard to tell if this is right or wrong, usually need human label and test data\n",
    "#right now we don't have a lot of data (only 150 rows), knn.fit takes a long time, and usually run on gpu, or cloud, called model persistence, want to save the function to a file and use it for predictions\n",
    "#use scikit learn\n",
    "from sklearn.externals import joblib\n",
    "model = joblib.dump(knn,'mlbrain.joblib') #stores model in binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 0, 0, 0, 2, 1, 1, 2, 0, 2, 1, 2, 2, 1, 0, 2, 1, 0, 1, 2,\n",
       "       1, 0, 1, 0, 2, 1, 0, 0, 0, 1, 2, 1, 1, 1, 1, 0, 2, 2, 2, 1, 1, 2,\n",
       "       0, 1, 2, 0, 0, 2, 1, 2, 0, 2, 2, 1, 2, 0, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now can do this, allows business owners to train models as they get more complex\n",
    "model = joblib.load('mlbrain.joblib')\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:  ['versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "sample = [[3,5,4,2],[2,3,5,4]]\n",
    "predictions = model.predict(sample)\n",
    "pred_species = [iris.target_names[p] for p in predictions]\n",
    "print(\"predictions: \", pred_species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
